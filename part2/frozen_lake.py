import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import pickle
import random

# --- å‰µæ„å¯¦ä½œï¼šæˆåŠŸè·¯å¾‘è¨˜æ†¶é«” (PathMemory) ---
class PathMemory:
    """
    è¿½è¹¤ä¸¦å„²å­˜æ‰€æœ‰æˆåŠŸçš„ State-Action åºåˆ—ã€‚
    """
    def __init__(self, capacity=200): # ç¨å¾®åŠ å¤§è¨˜æ†¶é«”
        self.success_paths = []
        self.current_trajectory = [] 
        self.capacity = capacity

    def record_step(self, state, action, reward, next_state):
        self.current_trajectory.append((state, action, reward, next_state))

    def finalize_path(self, is_success):
        if is_success:
            if len(self.success_paths) >= self.capacity:
                self.success_paths.pop(0) 
            self.success_paths.append(list(self.current_trajectory))
        self.current_trajectory = []

    def sample_batch(self, batch_size=32):
        if not self.success_paths:
            return None
        path_index = random.randrange(len(self.success_paths))
        path = self.success_paths[path_index]
        batch = random.choices(path, k=min(batch_size, len(path)))
        return batch

# -------------------------------------------------------------------------
# Part 2 æ ¸å¿ƒå‡½æ•¸
# -------------------------------------------------------------------------

def print_success_rate(rewards_per_episode):
    total_episodes = len(rewards_per_episode)
    success_count = np.sum(rewards_per_episode)
    success_rate = (success_count / total_episodes) * 100
    print(f"âœ… Success Rate: {success_rate:.2f}% ({int(success_count)} / {total_episodes} episodes)")
    return success_rate

def run(episodes, is_training=True, render=False, min_exp_rate=0.001):

    env = gym.make('FrozenLake-v1', map_name="8x8", is_slippery=True, render_mode='human' if render else None)
    
    memory = PathMemory(capacity=100) 
    REPLAY_BATCH_SIZE = 32 # ç¨å¾®é™ä½ batch size é¿å…éåº¦æ“¬åˆèˆŠè·¯å¾‘

    if is_training:
        q = np.zeros((env.observation_space.n, env.action_space.n)) 
    else:
        try:
            with open('frozen_lake8x8.pkl', 'rb') as f:
                q = pickle.load(f)
        except FileNotFoundError:
            print("è©•ä¼°éŒ¯èª¤: æ‰¾ä¸åˆ° frozen_lake8x8.pkl æª”æ¡ˆ")
            return 0.0

    # === é—œéµä¿®æ”¹ 1: å­¸ç¿’ç‡èª¿ä½ï¼ŒæŠ˜æ‰£å› å­ç¨å¾®é™ä½ ===
    learning_rate_a = 0.087  # ä½å­¸ç¿’ç‡é©åˆéš¨æ©Ÿç’°å¢ƒ (Stochastic Environment)
    discount_factor_g = 0.99 # ä¿æŒé«˜ç»é çŸš
    
    epsilon = 1 
    
    # === é—œéµä¿®æ”¹ 2: å‹•æ…‹è¨ˆç®—è¡°æ¸›ç‡ ===
    # ç¢ºä¿ epsilon åœ¨è¨“ç·´çš„ 80% éšæ®µæ‰é™åˆ°æœ€ä½ï¼Œçµ¦äºˆå……è¶³æ¢ç´¢æ™‚é–“
    epsilon_decay_rate = 1 / (episodes * 0.8) 
    
    rng = np.random.default_rng() 
    rewards_per_episode = np.zeros(episodes)

    for i in range(episodes):
        state = env.reset()[0]
        terminated = False 
        truncated = False 

        while(not terminated and not truncated):
            
            if is_training and rng.random() < epsilon:
                action = env.action_space.sample() 
            else:
                action = np.argmax(q[state,:])

            new_state, reward, terminated, truncated, _ = env.step(action)
            
            memory.record_step(state, action, reward, new_state)

            if is_training:
                # æ¨™æº– Q-Learning æ›´æ–°
                max_q_next = np.max(q[new_state,:])
                q[state,action] += learning_rate_a * (
                    reward + discount_factor_g * max_q_next - q[state,action]
                )

            state = new_state
        
        # --- Episode çµæŸ ---
        is_success = (reward == 1.0)
        memory.finalize_path(is_success)

        # --- æˆåŠŸç¶“é©—å›æ”¾ (Success Experience Replay) ---
        # æ³¨æ„ï¼šæˆ‘å€‘ä¿æŒè¼ƒä½çš„å­¸ç¿’ç‡é€²è¡Œå›æ”¾ï¼Œé¿å…ã€Œå€–å­˜è€…åå·®ã€éé‡
        if is_training and memory.success_paths and is_success: 
            # ä¿®æ”¹ç­–ç•¥ï¼šåªæœ‰åœ¨ã€Œå‰›å¥½æˆåŠŸã€æˆ–ã€Œæ¯éš”å¹¾æ¬¡ã€æ™‚æ‰é‡æ”¾ï¼Œæˆ–è€…ä¿æŒæ¯æ¬¡é‡æ”¾
            # é€™è£¡ä¿æŒæ¯æ¬¡é‡æ”¾ï¼Œä½†ä¾è³´ä½å­¸ç¿’ç‡ä¾†ç©©å®š
            batch = memory.sample_batch(REPLAY_BATCH_SIZE)
            if batch:
                for s, a, r, ns in batch:
                    max_q_next = np.max(q[ns,:])
                    q[s,a] += learning_rate_a * (
                        r + discount_factor_g * max_q_next - q[s,a]
                    )
        
        # è¡°æ¸› epsilon
        epsilon = max(epsilon - epsilon_decay_rate, min_exp_rate)

        # === é—œéµä¿®æ”¹ 3: ç§»é™¤å­¸ç¿’ç‡å¼·åˆ¶æ­¸é›¶çš„é‚è¼¯ ===
        # è®“ agent åœ¨ epsilon å¾ˆä½æ™‚ç¹¼çºŒå¾®èª¿ Q-table

        if reward == 1:
            rewards_per_episode[i] = 1
        
        # æ¯ 1000 æ¬¡å°å‡ºé€²åº¦ï¼Œé¿å…çœ‹èµ·ä¾†åƒç•¶æ©Ÿ
        if (i+1) % 5000 == 0 and is_training:
             print(f"Episode {i+1}: ç›®å‰ Epsilon {epsilon:.4f}")

    env.close()

    sum_rewards = np.zeros(episodes)
    for t in range(episodes):
        sum_rewards[t] = np.sum(rewards_per_episode[max(0, t-100):(t+1)])
    plt.plot(sum_rewards)
    plt.savefig('frozen_lake8x8.png')
    
    if is_training == False:
        print_success_rate(rewards_per_episode)

    if is_training:
        with open("frozen_lake8x8.pkl","wb") as f:
            pickle.dump(q, f)

if __name__ == '__main__':
    
    # è¨­ç½® min_exploration_rate
    MIN_RATE = 0.001 

    print(f"--- ğŸš€ Frozen Lake (é«˜æº–ç¢ºåº¦å„ªåŒ–ç‰ˆ) é‹è¡Œ ---")
    print(f"æ ¸å¿ƒç­–ç•¥: ä½å­¸ç¿’ç‡ (0.1) + é•·æ™‚é–“è¨“ç·´ + ç¶“é©—å›æ”¾")
    print("-" * 35)
    
    # === é—œéµä¿®æ”¹ 4: å¢åŠ è¨“ç·´æ¬¡æ•¸ ===
    # 8x8 Slippery éå¸¸é›£æ”¶æ–‚ï¼Œå»ºè­°è‡³å°‘ 25000 ~ 30000 æ¬¡
    print("é–‹å§‹è¨“ç·´ (15,000 episodes)...")
    run(15000, is_training=True, render=False, min_exp_rate=MIN_RATE)

    print("\né–‹å§‹è©•ä¼° (1,000 episodes)...")
    run(1000, is_training=False, render=False, min_exp_rate=MIN_RATE)
